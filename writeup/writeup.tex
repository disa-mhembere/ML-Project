
\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2010}
\usepackage{paralist}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\mathchardef\mhyp="2D

\title{Identifying Anomalous Activity the Enron Email \\
				Corpus using Glocal Multivariate Graph Statistics}

\author{Disa Mhembere\\
  Department of Computer Science\\
  Johns Hopkins University\\
  {\tt dmhembe1@jhu.edu}
  \And
  Kunal Lillaney \\
  Department of Computer Science\\
  Johns Hopkins University\\
  {\tt lillaney@jhu.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Graphs are quickly emerging as a leading abstraction for the representation of data
flow and interactions within networks. One high-interest area of study deals with 
the use of graph theory to detect attacks and anomalous activity in networks 
\cite{priebe2005scan,park2009anomaly,park2013anomaly}.
Additionally, the use of machine learning to detect threats within networks 
has become a topic of particular interest 
\cite{mahoney2003machine,shon2005machine,sommer2010outside,shon2007hybrid} within
cyber security and machine learning circles alike.

The use of glocal multivariate graph statistics \cite{mhembere2013computing} as 
a supplementary source of multi-dimensional features for anomalous network detection,
has not been explored \textit{to our knowledge}. Such statistics can expose 
otherwise-latent topological attributes within networks that have the potential 
to improve anomalous activity detection within a network.

We use of such statistics to generate novel features for use within
classification tasks for the Enron corpus of emails \cite{enronrepo2009}.
We hope to show that the use of such statistics can help to identify periods of
anomalous activity and associated actors.
\end{abstract}


\section{Introduction}
Graphs are an intuitive high-level abstraction of interaction in any setting where
there is `information flow' and multiple points where its exchange may occur. TODO

\section{Background}
Todo

\section{Design}
This section describes our choice of glocal graph statistics and discusses their suitability
to the task of detection of anomalous activity and actors in the Enron email corpus.
We further discus our choice of Machine Learning algorithms. Lastly, we discuss our metrics
of success and evaluation for our implementations.

\subsection{Graph Statistics}
Carey et al \cite{priebe2005scan} showed that through the use of the a glocal graph 
statistic (namely the Scan statistic), one could successfully detect anomalous activity.
We chose to look further and ask whether traditionally topologically-encoded graph statistics would
be even better suited to the task of classifying both periods of anomaly and those
most highly associated with the anomaly.

For consistency, let $G$ be a graph, with vertex set $V$ and edges $u \sim v \in E$ (the set of all $m$ edges). Let $e_{ij}$ be a directed edge from $v_i$ to $v_j$. Let $l(u,v)$ be the edge distance between vertices $u, v$. Let the $k$-hop \emph{neighborhood} of a graph $G$ around vertex $v$ be denoted by $N_k[v;G]$, where $N_k[v;G] = \{u \in V:l(u,v) \le k\}$. Let $\eta_i$ denote the neighbors for $v_i$. Finally, let the $C\mhyp k$ denote the size of a clique as $k$, where a clique $C \subset V$ s.t $\forall v,u \in C\ \exists\ v \sim u$.


 
As such we use the following statistics:
\begin{itemize}
\item Local in-degree: $deg_{in} = \sum_{j \in \eta_i} v_{ji}\  \forall\ v \in V$
\item Local out-degree: $deg_{out} = \sum_{j \in \eta_i} v_{ij}\  \forall\ v \in V$ 
\item Local triangle count: $\Delta = C\mhyp3 \  \forall\ v \in V$
\item Local scan-1 statistic: \\ $SS = \sum e_{ij} \in N_1[v_i;G] \forall\ v \in V$
\item Local clustering coefficient: \\ Let, $deg = deg_{in} + deg_{out}$ 
\\ $CC = \frac{2\Delta}{deg (deg-1)}$
\item Eigen-spectral decomposition: $\lambda = \mathbf{vA}$, where $\lambda$ are the
eigenvalues, $\mathbf{v}$ are the eigenvectors, and $\mathbf{A}$ is the adjacency matrix of $G$.

\end{itemize}
These statistics have proved extremely statistically powerful in summarizing graph
topology \cite{pao2011statistical}. All but Eigen-spectral decomposition provide
connectivity and interaction information from one vertex to the next. These will
logically be the most informative in our case since we hypothesize there is a strong
correlation between topological non-homogeneity within graphs and anomalous/malicious
activity. The spectral decomposition actually produces a compressed relationship mapping
between each pair of vertices in the graph. As such, it follows that this is an obvious
candidate for determining when atypical patterns occur in the graph.

\subsection{Machine Learning Tools}
\hspace*{10pt}\textit{Anomalous time periods:} \\
One aim is to determine \textit{when} atypical communications occur. We choose to
use weighted k-Nearest Neighbors (wKNN) to identify malicious actors in the network
at different time points since we conjecture the features of malicious users will
have high relative `similarity', where we use feature-vector squared euclidean distance
as our metric for similarity. i.e.,

$d = formula$


\textit{Anomalous actors:} \\
The other aim is to deterrine \textit{who} were those who caused the anomalous
activity during these periods of unrest. Literature \cite{shon2007hybrid,shon2005machine}
leads us to believe that Support Vector Machines (SVM)s are a suitable from such 
a classification task. 


\section{Methods}
In this section we describe both data acquisition phase and the learning algorithm
implementation and the challenges we faced in the pursuit of both.

\subsection{Data Acquisition}
To realize our goals we obtain raw data from two sources:
\begin{inparaenum}[\itshape(i)]
\item http://www.cs.cmu.edu/$\sim$./enron/. This resource contains over 0.5 million
emails from senior management within Enron and 
\item http://cis.jhu.edu/parky/Enron/enron.html. This is a resource from where we obtain
time-series graphs of Enron email activity for a 189 week period between 1998 and 2002.
Here each graph represents a week of email interaction between $\sim$150 to executives
at Enron.
\end{inparaenum}

\subsubsection{Raw email Feature Extraction}

\subsubsection{Challenges: Raw email Feature Extraction}
We faced several:

\begin{itemize}
\item Several email addresses to a single user
\item Non-uniform email addresses generated in raw text
\item Auto-generated emails
\end{itemize}



\subsubsection{Time Series Feature Extraction}
Todo
\subsubsection{Challenges: Feature Extraction}
Todo
\subsubsection{Regularization}
Todo

\section{Experiment Design}
Todo
\section{Results}
Todo

\section{Comparison to proposal}
We did achieve all we set out to accomplish!

\section{Conclusion}
Todo



\section*{Bibliography}
\bibliographystyle{IEEEtran}
\bibliography{writeup.bib}
%\bibliographystyle{acl}
%\bibliography{references.bib}
\end{document}
